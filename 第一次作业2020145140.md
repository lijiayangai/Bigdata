## 1.1 [MapReduce](https://so.csdn.net/so/search?q=MapReduce&spm=1001.2101.3001.7020)是什么

　　[Hadoop](https://so.csdn.net/so/search?q=Hadoop&spm=1001.2101.3001.7020) MapReduce是一个软件框架，基于该框架能够容易地编写应用程序，这些应用程序能够运行在由上千个商用机器组成的大集群上，并以一种可靠的，具有容错能力的方式并行地处理上TB级别的海量数据集。这个定义里面有着这些关键词，***\*一是软件框架，二是并行处理，三是可靠且容错，四是大规模集群，五是海量数据集。\****

## 1.2 MapReduce做什么

　　MapReduce擅长处理大数据，它为什么具有这种能力呢？这可由MapReduce的设计思想发觉。MapReduce的思想就是“**分而治之**”。

　　（1）***\*Mapper\**负责“\**分\**”**，即把复杂的任务分解为若干个“简单的任务”来处理。“简单的任务”包含三层含义：

一是数据或计算的规模相对原任务要大大**缩小**；二是***\*就近计算原则\****，即任务会分配到存放着所需数据的节点上进行计算；三是这些小任务***\*可以并行计算，彼此间几乎没有依赖\****关系。

　　（2）***\*Reducer\****负责对map阶段的***\*结果进行汇总\****。至于需要多少个Reducer，用户可以根据具体问题，通过在mapred-site.xml配置文件里设置参数mapred.reduce.tasks的值，缺省值为1。

# 二、Hadoop中的MapReduce框架

​      一个MapReduce作业通常会把输入的[数据集](https://so.csdn.net/so/search?q=数据集&spm=1001.2101.3001.7020)切分为若干独立的数据块，由Map任务以完全并行的方式去处理它们。

​      框架会对Map的输出先进行排序，然后把结果输入给Reduce任务。通常作业的输入和输出都会被存储在文件系统中，整个框架负责任务的调度和监控，以及重新执行已经关闭的任务。

​      通常，MapReduce框架和分布式文件系统是运行在一组相同的节点上，也就是说，计算节点和存储节点通常都是在一起的。这种配置允许框架在那些已经存好数据的节点上高效地调度任务，这可以使得整个集群的网络带宽被非常高效地利用。

## 2.1 MapReduce框架的组成



　（1）JobTr![img](http://images.cnitblog.com/blog/381412/201312/21154930-a8557192283247449ce5a4adabc7585d.png)acker

　　JobTracker负责调度构成一个作业的所有任务，这些任务分布在不同的TaskTracker上（由上图的JobTracker可以看到2 assign map 和 3 assign reduce）。你可以将其理解为公司的项目经理，项目经理接受项目需求，并划分具体的任务给下面的开发工程师。

　　（2）TaskTracker

　　TaskTracker负责执行由JobTracker指派的任务，这里我们就可以将其理解为开发工程师，完成项目经理安排的开发任务即可。

## 2.2 MapReduce的输入输出

　　MapReduce框架运转在**<key,value>**键值对上，也就是说，框架把作业的输入看成是一组<key,value>键值对，同样也产生一组<key,value>键值对作为作业的输出，这两组键值对有可能是不同的。

一个MapReduce作业的输入和输出类型如下图所示：可以看出在整个流程中，会有三组<key,value>键值对类型的存在。

![img](http://images.cnitblog.com/blog/381412/201502/121334513709082.png)

# MapReduce的数据流

**分片、格式化数据源**
输入 Map 阶段的数据源，必须经过分片和格式化操作。

分片操作：指的是将源文件划分为大小相等的小数据块( Hadoop 2.x 中默认 128MB )，也就是分片( split )，
Hadoop 会为每一个分片构建一个 Map 任务，并由该任务运行自定义的 map() 函数，从而处理分片里的每一条记录;
格式化操作：将划分好的分片( split )格式化为键值对<key,value>形式的数据，其中， key 代表偏移量， value 代表每一行内容。
**执行 MapTask**
每个 Map 任务都有一个内存缓冲区(缓冲区大小 100MB )，输入的分片( split )数据经过 Map 任务处理后的中间结果会写入内存缓冲区中。
如果写人的数据达到内存缓冲的阈值( 80MB )，会启动一个线程将内存中的溢出数据写入磁盘，同时不影响 Map 中间结果继续写入缓冲区。
在溢写过程中， MapReduce 框架会对 key 进行排序，如果中间结果比较大，会形成多个溢写文件，最后的缓冲区数据也会全部溢写入磁盘形成一个溢写文件，如果是多个溢写文件，则最后合并所有的溢写文件为一个文件。
**执行 Shuffle 过程**
MapReduce 工作过程中， Map 阶段处理的数据如何传递给 Reduce 阶段，这是 MapReduce 框架中关键的一个过程，这个过程叫作 Shuffle 。
Shuffle 会将 MapTask 输出的处理结果数据分发给 ReduceTask ，并在分发的过程中，对数据按 key 进行分区和排序。

**执行 ReduceTask**
输入 ReduceTask 的数据流是<key, {value list}>形式，用户可以自定义 reduce()方法进行逻辑处理，最终以<key, value>的形式输出

**MapReduce任务运行流程**

**一个MR任务执行流程**
Map端
1.InputFormat(默认实现类为TextInputFormat)读取外部数据，调用RecordReader.read()方法读取，返回k,v键值对；
2.读取的k,v键值对传入map方法中，在其中执行响应逻辑；
3.调用context.write()方法，outputCollector会将map()计算结果写入环形缓存区；
4.环形缓存区是一个环形数组（一般为100M）,缓存达到阈值（80%），spiller组件将内容将溢出到磁盘，溢写过程中map输出仍会写入环形缓存区，当环形缓存区填满后会阻塞直到溢出完成。
5.溢写过程中会按照定义分区（默认为HashPartition），并按key.comapreTo()排序，若有combiner则会执行combiner。这个过程生成的小文件仍在map端机器上。
6.将小文件合并，变成分区内有序的大文件，这会再一次调用combiner。
7.Reducer会按照分区从响应map task中拉去数据

**Reduce端**
1.因为reduce任务需要集群上若干个map的数据，所以每个map任务完成便开始并行（默认为5）拉去数据；（若map端输出很小，会被复制到reduce任务JVM的内存，当达到阈值时会被溢出到磁盘）；
2.通过GroupingComparator()分辨同一组数据，将其发给reduce(k,iterator)，多个数据合并成一组时只会取第一个kay；
3.调用context.wriite()方法，会让OutPutFormat调用write()方法将处理结果存入数据仓库，写的只有一个分区的文件。

**YARN(MR2)工作机制**
MRv1存在各种局限：
扩展性差：在 MRv1 中，JobTracker 同时兼备了资源管理和作业控制两个功能，这 成为系统的一个最大瓶颈，严重制约了 Hadoop 集群扩展性。
可靠性差：MRv1 采用了 master/slave 结构，其中，master 存在单点故障问题，一旦 它出现故障将导致整个集群不可用。
资源利用率低：MRv1 采用了基于槽位的资源分配模型，槽位是一种粗粒度的资源 划分单位，通常一个任务不会用完槽位对应的资源，且其他任务也无法使用这些空 闲资源。此外，Hadoop 将槽位分为 Map Slot 和 Reduce Slot 两种，且不允许它们之 间共享，常常会导致一种槽位资源紧张而另外一种闲置（比如一个作业刚刚提交时， 只会运行 Map Task，此时 Reduce Slot 闲置）。
无法支持多种计算框架：随着互联网高速发展，MapReduce 这种基于磁盘的离线计 算框架已经不能满足应用要求，从而出现了一些新的计算框架，包括内存计算框架、 流式计算框架和迭代式计算框架等，而 MRv1 不能支持多种计算框架并存。YARN将JobTracker资源管理和作业控制两个功能分别交给ResourceManager与MRAppMaster
工作角色：
客户端:提交作业
ResourceManager：资源调度(主要包括AppMaster和调度器（Scheduler）)
NodeManger：启动和监视节点的计算容器
MRApplicationMaster：协调运行作业中的任务


![img](https://img-blog.csdnimg.cn/202009131733401.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzQxODM5MA==,size_16,color_FFFFFF,t_70#pic_center)